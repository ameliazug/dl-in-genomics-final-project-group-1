# -*- coding: utf-8 -*-
"""M2A2 Training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EIh_sMrHpkdPj1pixzMei9Ek5QQx2oLa
"""

# --- Clean and compatible environment setup ---
!pip install --upgrade --force-reinstall numpy==1.23.5 scipy==1.10.1 jax==0.4.13 jaxlib==0.4.13 --quiet

# Restart runtime after install (Colab will auto-kick you out, this is expected)
import os
os.kill(os.getpid(), 9)

import numpy as np
import pandas as pd
import h5py

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error

from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import (
    Input, Conv1D, Conv2D, Reshape, Bidirectional, LSTM, Dense, Dropout, LeakyReLU, Flatten
)
from tensorflow.keras.constraints import MaxNorm
from tensorflow.keras.optimizers import Adadelta
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

from google.colab import drive

"""import numpy as np
import pandas as pd
import h5py
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import (
    Input, Conv1D, Conv2D, Reshape, Bidirectional, LSTM, Dense, Dropout, LeakyReLU
)
from tensorflow.keras.constraints import MaxNorm
from tensorflow.keras.optimizers import Adadelta
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

from sklearn.metrics import r2_score, mean_absolute_error
from google.colab import drive
import pandas as pd
import h5py
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import (
    Input, Conv1D, Conv2D, Reshape, Bidirectional, LSTM, Dense, Dropout, LeakyReLU
)
from tensorflow.keras.constraints import MaxNorm
from tensorflow.keras.optimizers import Adadelta
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

from sklearn.metrics import r2_score, mean_absolute_error
from google.colab import drive"""

drive.mount('/content/drive/')

#Retrieve all relevant filepaths below:

processed_data_filepath = '/content/drive/MyDrive/Semester06/CSCI2952G/Project/Processed_Data/' #Replace with your drive's filepath as needed.

methyl_list1 = [processed_data_filepath + 'A549/methyl_enrichment.h5',
                processed_data_filepath + 'GM12878/methyl_enrichment.h5',
                processed_data_filepath + 'GM23248/methyl_enrichment.h5',
                processed_data_filepath + 'H1/methyl_enrichment.h5',
                processed_data_filepath + 'HeLa-S3/methyl_enrichment.h5',
                processed_data_filepath + 'HepG2/methyl_enrichment.h5',
                processed_data_filepath + 'K562/methyl_enrichment.h5',
                processed_data_filepath + 'OCI-LY7/methyl_enrichment.h5',
                processed_data_filepath + 'SKNSH/methyl_enrichment.h5'
                ]
methyl_list2 = [processed_data_filepath + 'A549/methyl_enrichment2.h5',
                processed_data_filepath + 'GM12878/methyl_enrichment2.h5',
                processed_data_filepath + 'GM23248/methyl_enrichment2.h5',
                processed_data_filepath + 'H1/methyl_enrichment2.h5',
                processed_data_filepath + 'HeLa-S3/methyl_enrichment2.h5',
                processed_data_filepath + 'HepG2/methyl_enrichment2.h5',
                processed_data_filepath + 'K562/methyl_enrichment2.h5',
                processed_data_filepath + 'OCI-LY7/methyl_enrichment2.h5',
                processed_data_filepath + 'SKNSH/methyl_enrichment2.h5'
                ]
acetyl_list1 = [processed_data_filepath + 'A549/acetyl_enrichment.h5',
                processed_data_filepath + 'GM12878/acetyl_enrichment.h5',
                processed_data_filepath + 'GM23248/acetyl_enrichment.h5',
                processed_data_filepath + 'H1/acetyl_enrichment.h5',
                processed_data_filepath + 'HeLa-S3/acetyl_enrichment.h5',
                processed_data_filepath + 'HepG2/acetyl_enrichment.h5',
                processed_data_filepath + 'K562/acetyl_enrichment.h5',
                processed_data_filepath + 'OCI-LY7/acetyl_enrichment.h5',
                processed_data_filepath + 'SKNSH/acetyl_enrichment.h5'
                ]

acetyl_list2 = [processed_data_filepath + 'A549/acetyl_enrichment2.h5',
                processed_data_filepath + 'GM12878/acetyl_enrichment2.h5',
                processed_data_filepath + 'GM23248/acetyl_enrichment2.h5',
                processed_data_filepath + 'H1/acetyl_enrichment2.h5',
                processed_data_filepath + 'HeLa-S3/acetyl_enrichment2.h5',
                processed_data_filepath + 'HepG2/acetyl_enrichment2.h5',
                processed_data_filepath + 'K562/acetyl_enrichment2.h5',
                processed_data_filepath + 'OCI-LY7/acetyl_enrichment2.h5',
                processed_data_filepath + 'SKNSH/acetyl_enrichment2.h5'
                ]

#Methods below are used as a check to inspect the above preprocessed data:

def inspect_data(file_path: str):
  with h5py.File(file_path, 'r') as f:
    print("Top-level keys (datasets/groups):")
    for key in f.keys():
        data = f[key]
        if isinstance(data, h5py.Dataset):
            print(f"Dataset: {key}, Shape: {data.shape}, Dtype: {data.dtype}")
        else:
            print(f"Group: {key} (may contain nested datasets)")


file = processed_data_filepath + 'A549/methyl_enrichment.h5'
inspect_data(file)

#Process data into overall inputs here:

def load_training_data(h5_file):
    with h5py.File(h5_file, 'r') as f:
        X = np.array(f['FeatureInput'])[:, :, :, [0, 1, 3]] #Drop the last feature, since it is not included in the M2A paper...
        Y = np.array(f['log2_ChipDivInput']).astype(float)
    return X, Y


def combine_h5_list(file_list):
    X_parts, Y_parts = [], []
    for fp in file_list:
        Xp, Yp = load_training_data(fp)
        X_parts.append(Xp)
        Y_parts.append(Yp)
    X_all = np.concatenate(X_parts, axis=0)
    Y_all = np.concatenate(Y_parts, axis=0)
    return X_all, Y_all

#TESTING REPLICATE CONSISTENCY HERE, by making heat maps of replicate 1 vs replicate 2 (and calculating r2) for each cell line:

for rep1, rep2 in zip(acetyl_list1, acetyl_list2):
    _, Y1 = load_training_data(rep1)
    _, Y2 = load_training_data(rep2)
    r2 = r2_score(Y1, Y2)
    print(f'Acetylation r^2 {rep1.replace(processed_data_filepath, "")}: {r2}')


for rep1, rep2 in zip(methyl_list1, methyl_list2):
    _, Y1 = load_training_data(rep1)
    _, Y2 = load_training_data(rep2)
    r2 = r2_score(Y1, Y2)
    print(f'Methylation r^2 {rep1.replace(processed_data_filepath, "")}: {r2}')


"""
RESULTS:

0.9347618151637055
0.0798063791735032
0.8285030522783727
-0.42519509681821543
0.8800857694182009
0.559526872467845
0.7305401796262327
0.8283268154426386
0.7523384642151644



Q: your UMAPs look more separated than I've seen in other papers.... Why do you think that is?
"""

"""

methyl_list1 = [processed_data_filepath + 'A549/methyl_enrichment.h5', #BEST
                processed_data_filepath + 'GM12878/methyl_enrichment.h5', #BAD
                processed_data_filepath + 'GM23248/methyl_enrichment.h5',
                processed_data_filepath + 'H1/methyl_enrichment.h5', #BAD
                processed_data_filepath + 'HeLa-S3/methyl_enrichment.h5',
                processed_data_filepath + 'HepG2/methyl_enrichment.h5',
                processed_data_filepath + 'K562/methyl_enrichment.h5',
                processed_data_filepath + 'OCI-LY7/methyl_enrichment.h5',
                processed_data_filepath + 'SKNSH/methyl_enrichment.h5'
                ]"""

#Call above methods to get input files for each set of data (methyl rep1, methyl rep2, acetyl rep1, acetyl rep2):
methyl_rep1_data = combine_h5_list(methyl_list1)
methyl_rep2_data = combine_h5_list(methyl_list2)
acetyl_rep1_data = combine_h5_list(acetyl_list1)
acetyl_rep2_data = combine_h5_list(acetyl_list2)

#Create M2A2 Architecture below:

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, LeakyReLU
from tensorflow.keras.constraints import MaxNorm

def build_m2a2_model(input_shape):
    """
    This model....
    """
    inputs = Input(shape=input_shape)
    x = Conv2D(64, (1, 3), padding='same', kernel_constraint=MaxNorm(3))(inputs)
    x = LeakyReLU(alpha=0.1)(x)
    x = Conv2D(128, (1, 3), padding='same', kernel_constraint=MaxNorm(3))(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = Conv2D(256, (1, 3), padding='same', kernel_constraint=MaxNorm(3))(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = Reshape((input_shape[0] * input_shape[1], -1))(x)
    x = Bidirectional(LSTM(64, return_sequences=False))(x)
    x = Dense(128, kernel_constraint=MaxNorm(3))(x)
    x = LeakyReLU(alpha=0.1)(x)
    x = Dropout(0.3)(x)
    x = Dense(64, kernel_constraint=MaxNorm(3))(x)
    x = LeakyReLU(alpha=0.1)(x)
    output_H3K27ac = Dense(1, name='H3K27ac')(x)
    output_H3K4me3 = Dense(1, name='H3K4me3')(x)
    return Model(inputs=inputs, outputs=[output_H3K27ac, output_H3K4me3])

#Create M2A Architecture below:
def build_m2a_model(input_shape):

    inputs = Input(shape=input_shape)
    x = Conv2D(filters=32, kernel_size=(1, 3), activation=LeakyReLU(alpha=0.1), padding='same', kernel_constraint=MaxNorm(3))(inputs)
    x = Conv2D(filters=64, kernel_size=(1, 3), activation=LeakyReLU(alpha=0.1), padding='same', kernel_constraint=MaxNorm(3))(x)
    x = Conv2D(filters=128, kernel_size=(1, 3), activation=LeakyReLU(alpha=0.1), padding='same', kernel_constraint=MaxNorm(3))(x)

    x = Flatten()(x)

    # Fully Connected Layers
    x = Dense(256, activation=LeakyReLU(alpha=0.1), kernel_constraint=MaxNorm(3))(x)
    x = Dense(128, activation=LeakyReLU(alpha=0.1), kernel_constraint=MaxNorm(3))(x)

    # Output Layer
    # Only one output layer, for the specified histone modification.
    output_layer = Dense(1)(x)

    # Define the model
    model = Model(inputs=inputs, outputs=output_layer)

    return model

#Train model here:
target_histone = 'methyl'

def train_m2a_model(input_data, target_data):
  # 1. Input Shape
    input_shape = input_data.shape[1:]  # Exclude the batch dimension #CHECK THIS....

    # 2. Model Instantiation
    model = build_m2a_model(input_shape)

    # 3. Optimizer
    optimizer = Adadelta()  # You can specify learning rate and other parameters here

    # 4. Loss Function
    loss_function = 'mean_squared_error'  # Or another suitable regression loss

    # 5. Metrics
    metrics = ['mean_absolute_error']  # Or other relevant metrics

    # 6. Compile the Model
    model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)

    # 7. Data Splitting
    # A standard 80/20 training/validation split
    X_train, X_val, y_train, y_val = train_test_split(input_data, target_data, test_size=0.2, random_state=12) # Added random_state for reproducibility

    # 8. Batch Size
    batch_size = 512

    # 9. Early Stopping
    # To prevent overtraining
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,
        min_delta=0.0001,
        restore_best_weights=True,  # Important: Restore the best model state
        verbose=1  # Add verbose for more informative output
    )

    #Added by GPT to compute R^2
    #print(f"Training history keys: {history.history.keys()}")

    # --- NEW: compute R^2 on validation set ---
    y_val_pred = model.predict(X_val, batch_size=batch_size, verbose=0)
    # if y_val or y_val_pred are shape (n,1), flatten them
    y_true_flat = y_val.flatten()
    y_pred_flat = y_val_pred.flatten() if y_val_pred.ndim > 1 else y_val_pred
    r2 = r2_score(y_true_flat, y_pred_flat)
    print(f"\nValidation R²: {r2:.4f}")


    # 10. Training
    # Sample input was randomized before training (achieved by train_test_split with shuffling)
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=80,  # Maximum epochs
        batch_size=batch_size,
        callbacks=[early_stopping],
        verbose=1  # Add verbose for more informative output during training
    )

    # Print the keys in history.history to see what's available.
    print(f"Training history keys: {history.history.keys()}")



    # 11. Model Saving (Optional, but recommended)
    # Save the trained model for later use
    model.save(f'/content/drive/MyDrive/Semester06/CSCI2952G/Models/m2a_model_{target_histone}.h5')
    print(f"Trained model saved as m2a_model_{target_histone}.h5")

    return model, history

X, Y = load_training_data(processed_data_filepath + 'A549/methyl_enrichment.h5')
model, history = train_m2a_model(X,Y)

import matplotlib.pyplot as plt

#Generate plot:

def plot_training_history(history):
    # Extract metrics from history
    history_dict = history.history
    epochs = range(1, len(history_dict['loss']) + 1)

    # Plot Loss
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, history_dict['loss'], label='Training Loss')
    plt.plot(epochs, history_dict['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss')
    plt.legend()

    # Plot Metric (e.g., MAE)
    if 'mean_absolute_error' in history_dict:
        plt.subplot(1, 2, 2)
        plt.plot(epochs, history_dict['mean_absolute_error'], label='Training MAE')
        plt.plot(epochs, history_dict['val_mean_absolute_error'], label='Validation MAE')
        plt.xlabel('Epochs')
        plt.ylabel('Mean Absolute Error')
        plt.title('Training and Validation MAE')
        plt.legend()

    plt.tight_layout()
    plt.show()

plot_training_history(history)

!pip install tensorflow==2.13.0

import tensorflow as tf
target_histone = 'methyl'
# Force the custom_objects to be used during loading
model = load_model(f'/content/drive/MyDrive/Semester06/CSCI2952G/Models/m2a_model_{target_histone}.h5', custom_objects={'LeakyReLU': tf.keras.layers.LeakyReLU})

X, Y = combine_h5_list(methyl_list1)
X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=12)
y_val_pred = model.predict(X_val)
r2 = r2_score(y_val, y_val_pred) #CHECK THIS!!!!!
print(r2)

print(y_val.shape)

"""
NOTES:
1. for methylation and M2A, r^2 = 0.01918899789242623 on validation
"""

#TESTING TRAINED WEIGHTS HERE:
from keras.models import load_model
import os
from sklearn.metrics import mean_squared_error, mean_absolute_error

feature_file_path = processed_data_filepath + 'GM23248/methyl_enrichment.h5'
model_file_path = '/content/drive/MyDrive/Semester06/CSCI2952G/Project/Models/M2A2_model_weights.h5'



def parse_input(in_file):
    with h5py.File(in_file, mode="r") as f:
        cols = ["Transcript", "Gene", "GeneName", "Chr", "Start", "End", "Strand"]
        idx_list = ["transcript_id", "EnsmblID_G", "Gene", "Chr", "Start", "End", "Strand"]
        meta_df = pd.DataFrame(columns=cols)
        for col, idx in zip(cols, idx_list):
          raw_values = np.array(f[idx]).astype(str)
          meta_df[col] = pd.Series(raw_values).str.replace("b'", "", regex=True).str.replace("'", "", regex=True)
        test_data = np.array(f["FeatureInput"])[:, :, :, [0, 1, 3]]
        target_data = np.array(f["Y"]) if "Y" in f else None
    return meta_df, test_data, target_data

# -----------------------------------------
# Prediction + Metrics
def predict_and_evaluate(meta_df, test_data, targets, model_path):
    model = load_model(model_path)
    predictions = model.predict(test_data, batch_size=512, verbose=1)

    # If single output, wrap in a list for uniform handling
    if not isinstance(predictions, list):
        predictions = [predictions]
        output_names = [model.output_names[0]]
    else:
        output_names = model.output_names

    # Handle multiple outputs
    for i, pred in enumerate(predictions):
        col_name = f"Predicted_{output_names[i]}"
        meta_df[col_name] = pred.flatten()
        print(f"\n📊 Predictions for {col_name} (first 10 rows):")
        print(meta_df[[col_name]].head(10))

        # Print metrics if Y is available and matches output
        if targets is not None:
            if targets.ndim == 1 or targets.shape[1] == 1:
                y_true = targets.flatten()
            else:
                y_true = targets[:, i]
            y_pred = pred.flatten()

            print(f"\n📈 Performance Metrics for {col_name}:")
            print(f"MAE: {mean_absolute_error(y_true, y_pred):.4f}")
            print(f"MSE: {mean_squared_error(y_true, y_pred):.4f}")

# -----------------------------------------
# Run Everything

meta_df, X, Y = parse_input(feature_file_path)
predict_and_evaluate(meta_df, X, Y, model_file_path)

